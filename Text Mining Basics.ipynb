{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "User_restaurants_reviews = pd.read_csv('E:\\\\NLP data\\\\Dats sets\\\\User_Reviews\\\\User_Reviews\\\\User_restaurants_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing the data\n",
    "User_restaurants_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_restaurants_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data_tiny = User_restaurants_reviews[0:3] #using only top 3 records for example analysis\n",
    "user_data_tiny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Review', 'Sentiment'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data_tiny.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize #loading tokenizing function\n",
    "\n",
    "example_text = user_data_tiny[\"Review\"][0]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow...', 'Loved this place.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(example_text) #tokenizing the sentences using sent_tokenize\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', '...', 'Loved', 'this', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_text) #tokenizing the words using word_tokenize\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords #steps to remove a, an ,the, this, that, etc.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 't', 'haven', 'yourself', \"shouldn't\", 'up', 'ourselves', 'on', 'am', 'we', 'what', 'those', \"mustn't\", 've', 'it', \"hadn't\", 'more', 'hasn', 'been', 'doesn', 'but', 'who', 'until', 'where', 'not', 'yourselves', \"hasn't\", 'why', 'i', 'and', 'same', 'hadn', 'being', 'at', 'an', 'does', 'only', 'don', \"should've\", 'your', 'nor', 'the', \"shan't\", 'herself', 'off', \"you're\", \"doesn't\", 'here', 'there', 'needn', 'through', 'ain', 'yours', 'that', 'as', \"that'll\", 'which', 'whom', 'for', 'were', 'theirs', \"haven't\", 'then', 'weren', 'during', 'should', 'do', 'all', 'other', \"isn't\", 'when', 'own', 'me', \"wouldn't\", 'they', 'couldn', 'hers', \"couldn't\", 'd', 'of', 'ours', 'once', \"needn't\", 'having', 'has', 'very', 'about', 'if', 'no', 'because', 'from', \"wasn't\", \"she's\", 'did', 'down', 'she', 'after', 'both', \"weren't\", 'above', 'himself', 'against', 'or', 'had', 'be', 'into', 'is', 'can', \"you'd\", 're', 'this', 'won', 'how', 'while', \"didn't\", 'any', 'myself', 'few', 'just', 'under', 'you', 'mightn', 'are', \"don't\", \"you'll\", 'will', 'didn', 'below', 'm', 'over', 'wouldn', 'itself', 'aren', 'by', 's', 'isn', 'mustn', 'him', 'her', 'with', 'doing', 'his', 'have', 'most', 'too', 'our', \"won't\", 'o', 'before', 'further', 'll', 'in', \"aren't\", 'my', 'than', 'between', 'shan', 'themselves', 'a', \"you've\", 'its', 'them', 'wasn', 'their', 'y', 'ma', 'again', 'so', 'was', 'each', 'such', 'to', \"it's\", 'some', \"mightn't\", 'shouldn', 'now', 'out', 'he'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', '...', 'Loved', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [word for word in word_tokens if not word in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #using PorterStemmer to stem the words back to root word\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.\n"
     ]
    }
   ],
   "source": [
    "example_text1 = user_data_tiny[\"Review\"][1]\n",
    "print(example_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens1 = word_tokenize(example_text1)\n",
    "print(word_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'that', 'if', 'an', 'electr', 'slicer', 'is', 'use', 'the', 'blade', 'becom', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens = [stemmer.stem(word) for word in word_tokens1]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer #You can also use lemmatizer to keep meaningful words but it also has disadvantages.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "Lemmatized_tokens = [lemmatizer.lemmatize(word) for word in word_tokens1]\n",
    "print(Lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review22_text = User_restaurant_reviews[\"Review\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing currency - \n",
      "We ordered     Margaritas they couldnt get the machine to work because it was frozen so refunded our money \n"
     ]
    }
   ],
   "source": [
    "import re # using regex expressions to remove punctuations, email ids, etc.\n",
    "review22_text_cleaned = re.sub(r'\\W+|\\d+|_',' ', review22_text)\n",
    "print(\"Text after removing currency - \\n\" + review22_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text - \n",
      "We ordered 2 $.99 Margaritas - they couldnt get the machine to work because it was frozen so refunded our money.\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual text - \\n\" + review22_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
